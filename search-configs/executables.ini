[executables]
page_ifar_catalog = ${which:pycbc_ifar_catalog}
average_psd = ${which:pycbc_average_psd}
inj2hdf = ${which:pycbc_convertinjfiletohdf}
bank2hdf = ${which:pycbc_coinc_bank2hdf}
calculate_psd = ${which:pycbc_calculate_psd}
coinc = ${which:pycbc_coinc_findtrigs}
combine_statmap = ${which:pycbc_add_statmap}
distribute_background_bins = ${which:pycbc_distribute_background_bins}
fit_by_template = ${which:pycbc_fit_sngls_by_template}
fit_over_param = ${which:pycbc_fit_sngls_over_multiparam_tha}
foreground_censor = ${which:pycbc_foreground_censor}
hdfinjfind = ${which:pycbc_coinc_hdfinjfind}
hdf_trigger_merge = ${which:pycbc_coinc_mergetrigs}
inj_cut = ${which:pycbc_inj_cut}
injections = ${which:lalapps_inspinj}
inspiral = singularity:///usr/local/bin/pycbc_inspiral_tha
ligolw_combine_segments = ${which:ligolw_combine_segments}
llwadd = ${which:ligolw_add}
merge_psds = ${which:pycbc_merge_psds}
optimal_snr = ${which:pycbc_optimal_snr}
page_foreground = ${which:pycbc_page_foreground}
page_ifar = ${which:pycbc_page_ifar}
page_injections = ${which:pycbc_page_injtable}
page_segplot = ${which:pycbc_page_segplot}
page_segtable = ${which:pycbc_page_segtable}
page_vetotable = ${which:pycbc_page_vetotable}
plot_bank = ${which:pycbc_plot_bank_bins}
plot_binnedhist = ${which:pycbc_fit_sngls_binned}
plot_coinc_snrchi = ${which:pycbc_page_coinc_snrchi}
plot_foundmissed = ${which:pycbc_page_foundmissed}
plot_gating = ${which:pycbc_plot_gating}
plot_hist = ${which:pycbc_plot_hist}
plot_qscan = ${which:pycbc_plot_qscan}
plot_range = ${which:pycbc_plot_range}
plot_segments = ${which:pycbc_page_segments}
plot_sensitivity = ${which:pycbc_page_sensitivity}
plot_singles = ${which:pycbc_plot_singles_vs_params}
plot_snrchi = ${which:pycbc_page_snrchi}
plot_snrifar = ${which:pycbc_page_snrifar}
plot_spectrum = ${which:pycbc_plot_psd_file}
plot_harmonic_waveform = ${which:pycbc_plot_harmonic_waveform}
exclude_zerolag = ${which:pycbc_exclude_zerolag}
plot_throughput = ${which:pycbc_plot_throughput}
results_page = ${which:pycbc_make_html_page}
segment_query = ${which:ligolw_segment_query_dqsegdb}
segments_from_cats = ${which:ligolw_segments_from_cats_dqsegdb}
splitbank = ${which:pycbc_hdf5_splitbank}
statmap = ${which:pycbc_coinc_statmap}
statmap_inj = ${which:pycbc_coinc_statmap_inj}
strip_injections = ${which:pycbc_strip_injections}
tmpltbank = ${which:pycbc_geom_nonspinbank}
html_snippet = ${which:pycbc_create_html_snippet}
foreground_minifollowup = ${which:pycbc_foreground_minifollowup}
injection_minifollowup = ${which:pycbc_injection_minifollowup}
singles_minifollowup = ${which:pycbc_sngl_minifollowup}
page_injinfo = ${which:pycbc_page_injinfo}
page_coincinfo = ${which:pycbc_page_coincinfo}
page_snglinfo = ${which:pycbc_page_snglinfo}
plot_trigger_timeseries = ${which:pycbc_plot_trigger_timeseries}
single_template_plot = ${which:pycbc_single_template_plot}
single_template = ${which:pycbc_single_template}
plot_singles_timefreq = ${which:pycbc_plot_singles_timefreq}
plot_snrratehist = ${which:pycbc_page_snrratehist}
plot_waveform = ${which:pycbc_plot_waveform}
page_versioning = ${which:pycbc_page_versioning}

[pegasus_profile]
; This section contains default profile information for every job
; This is overriden by profile information set for specific job types

; This sets the initial memory footprint request, initial disk request and
; CPU request
; This is in MB
condor|+InitialRequestMemory = 1980
; This is in KB, take care!!
condor|+InitialRequestDisk = 2000001
condor|request_cpus = 1
; Jobs will be evicted after running for this long. We then try again with
; longer runtimes progressively to try and get the job to complete. This
; catches the common case that a job gets "stuck" but does allow long-running
; jobs to complete, albeit with more delay.
condor|+ExpectedMaxRunTime = 20000
; Use this to override the number of times a job is allowed to start before
; evicting it. If memory/disk requests are way too low, a job may have to
; start multiple times before completing
condor|+MaxJobStarts = 5
; Use the initial request unless the job has been evicted for using too much
; memory. In that case, ask for 50% more than the last resident memory use.
; If it wasn't evicted for too much memory we set the memory request to the
; previous usage. If this job is running for the first time, MemoryUsage is not
; defined and we use InitialRequestMemory, we also use InitialRequestMemory
; if it is larger than previous MemoryUsage. (Sorry this is convoluted!)
; This uses an intermediate variable for readability
condor|+MinimumMemoryNeeded = max({ifThenElse((isInteger(NumJobStarts) && (NumJobStarts > 0) && (MemoryUsage =!= UNDEFINED) && (MemoryUsage =!= ERROR)), MemoryUsage, 0), InitialRequestMemory})
condor|request_memory = ifThenElse( ((LastHoldReasonCode=?=21) || (LastHoldReasonCode=?=34)), int(1.5 * max({NumJobStarts-1, 1}) * MinimumMemoryNeeded), MinimumMemoryNeeded)
condor|request_disk = int(1.5 * DiskUsage + InitialRequestDisk)
; If the inspiral job has run for more than the expected max run time
; multiplied by the number of job starts, assume something is stuck and evict
; it.
#condor|periodic_hold = (JobStatus =?= 2) && ((CurrentTime - EnteredCurrentStatus) > (ExpectedMaxRunTime*NumJobStarts + 600))
condor|periodic_hold = (JobStatus =?= 2) && ((CurrentTime - EnteredCurrentStatus) > (ExpectedMaxRunTime*max({NumJobStarts-1, 1}) + 600))
; If the job has been held for understood reasons, release it and let the
; memory request bump or disk request bump take effect if needed
; 3 = The periodic hold condition was met.
; 21 = The job was put on hold by condor itself for some reason.
; 32 and 33 = The job exceeded disk usage (possibly!)
; 34 = Job exceeded memory limit
condor|periodic_release = ((HoldReasonCode =?= 26) || (HoldReasonCode =?= 21) || (HoldReasonCode =?= 34) || (HoldReasonCode =?= 32) || (HoldReasonCode =?= 33) || ((JobStatus =?= 5) && (HoldReasonCode =?= 3) && (NumJobStarts < MaxJobStarts))) && ((CurrentTime - EnteredCurrentStatus) > (300))

; If held for these reasons remove the job. This logs a failure, which we 
; sometimes want, and then goes to our standard retry options.
; 12/13 = Issues during condor transfer. Shouldn't often happen.
; 26 = The job was held too many times. Evict if this happens.
condor|periodic_remove = ((HoldReasonCode =?= 12) || (HoldReasonCode =?= 13) || (NumJobStarts >= 5)) && ((CurrentTime - EnteredCurrentStatus) > (300))

;Example configuration for a PyCBC workflow
;condor|max_retries = 1 
;condor|on_exit_remove = (NumJobCompletions > JobMaxRetries)

pycbc|installed = True
condor|accounting_group = ligo.dev.o4.cbc.bbh.pycbcoffline
condor|when_to_transfer_output = ON_SUCCESS
condor|success_exit_code = 0
pycbc|primary_site = condorpool_symlink

##Newly added


[pegasus_profile-condorpool_shared&pegasus_profile-condorpool_symlink&pegasus_profile-osg]
env|BEARER_TOKEN_FILE = $$(CondorScratchDir)/.condor_creds/scitokens.use
condor|use_oauth_services = scitokens
env|HDF5_USE_FILE_LOCKING = FALSE
env|LAL_DATA_PATH=/cvmfs/software.igwn.org/pycbc/lalsuite-extra/e02dab8c/share/lalsimulation

[pegasus_profile-osg]
; Specify 'DESIRED' or 'UNDESIRED' OSG remote sites for inspiral jobs
;condor|+UNDESIRED_SITES="Nebraska,NIKHEF"
env|GWDATAFIND_SERVER=datafind.ligo.org:443

[pegasus_profile-condorpool_symlink]
env|GWDATAFIND_SERVER=datafind.ldas.cit:80

[pegasus_profile-results_page]
pycbc|site = condorpool_shared


[pegasus_profile-inspiral]
pycbc|site = osg
condor|+InitialRequestDisk = 10000000

[pegasus_profile-merge_psds]
condor|+InitialRequestMemory = 15000
condor|+InitialRequestDisk = 10000000

[pegasus_profile-coinc]
condor|+InitialRequestMemory = 30000


[pegasus_profile-plot_range]
condor|+InitialRequestMemory = 20000

[pegasus_profile-plot_singles]
condor|+InitialRequestMemory = 30000

[pegasus_profile-plot_snrchi]
condor|+InitialRequestMemory = 100000

[pegasus_profile-plot_spectrum]
condor|+InitialRequestMemory = 10000

[pegasus_profile-plot_trigger_timeseries]
condor|+InitialRequestMemory = 50000
pegasus|clusters.size = 5

[pegasus_profile-singles_minifollowup]
condor|+InitialRequestMemory = 50000
dagman|priority = 500

[pegasus_profile-single_template]
condor|+InitialRequestMemory = 3000
pegasus|clusters.size = 5

[pegasus_profile-single_template_plot]
condor|+InitialRequestMemory = 3000
pegasus|clusters.size = 5

[pegasus_profile-statmap]
condor|+InitialRequestMemory = 50000
dagman|priority = 500

[pegasus_profile-statmap_inj]
condor|+InitialRequestMemory = 10000
dagman|priority = 500

[pegasus_profile-exclude_zerolag]
condor|+InitialRequestMemory = 10000
dagman|priority = 500

[pegasus_profile-combine_statmap]
condor|+InitialRequestMemory = 10000
dagman|priority = 500

[pegasus_profile-hdfinjfind]
condor|+InitialRequestMemory = 5000
dagman|priority = 500

[pegasus_profile-page_snglinfo]
condor|+InitialRequestMemory = 4000
# This option will cause pegasus to cluster these jobs together in groups of
# 5 jobs. This means that 5 page_snglinfo jobs will become one condor job.
# That condor job will run the 5 jobs in sequence. Pegasus will keep track
# of which jobs are complete on eviction (or similar) to avoid repetition.
pegasus|clusters.size = 5

[pegasus_profile-page_coincinfo]
pegasus|clusters.size = 5
condor|+InitialRequestMemory = 30000

[pegasus_profile-plot_waveform]
pegasus|clusters.size = 5

[pegasus_profile-plot_singles_timefreq]
pegasus|clusters.size = 5
condor|+InitialRequestMemory = 30000

[pegasus_profile-plot_qscan]
pegasus|clusters.size = 5

[pegasus_profile-plot_binnedhist]
condor|+InitialRequestMemory = 100000

[pegasus_profile-plot_coinc_snrchi]
condor|+InitialRequestMemory = 30000

[pegasus_profile-optimal_snr]
condor|request_cpus = ${optimal_snr|cores}
condor|+InitialRequestMemory = 10000
dagman|priority = 5000


[pegasus_profile-fit_by_template]
dagman|priority = 5000
condor|+InitialRequestMemory = 40000

[pegasus_profile-hdf_trigger_merge]
dagman|priority = 5000
condor|+InitialRequestMemory = 80000
condor|+InitialRequestDisk = 80000000

[pegasus_profile-calculate_psd]
condor|+InitialRequestMemory = 8000
condor|request_cpus = ${calculate_psd|cores}
dagman|priority = 5000
dagman|retry = 10

[pegasus_profile-page_injections]
condor|+InitialRequestMemory = 10000

[pegasus_profile-page_foreground]
condor|+InitialRequestMemory = 20000

[pegasus_profile-plot_hist]
condor|+InitialRequestMemory = 100000
;condor|+InitialRequestDisk = 200000000

[pegasus_profile-injection_minifollowup]
condor|+InitialRequestMemory = 40000


